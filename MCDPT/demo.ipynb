{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "396c0ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hthek\\Downloads\\mct\\MCDPT\\instruct_ReID.py:35: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 751 classes from checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hthek\\anaconda3\\envs\\tracking\\lib\\site-packages\\timm\\models\\registry.py:4: FutureWarning: Importing from timm.models.registry is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\hthek\\anaconda3\\envs\\tracking\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using stride: [16, 16], and patch number is num_y16 * num_x8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hthek\\anaconda3\\envs\\tracking\\lib\\site-packages\\transformers\\modeling_utils.py:460: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  return torch.load(checkpoint_file, map_location=\"cpu\")\n",
      "c:\\Users\\hthek\\Downloads\\mct\\MCDPT\\instruct_ReID.py:98: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  ckpt = torch.load(checkpoint_path, map_location='cpu')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded checkpoint weights\n"
     ]
    }
   ],
   "source": [
    "from instruct_ReID import get_model,extract_feature_from_model\n",
    "ins_ReID=get_model().eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72eb827e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from boxmot import BoostTrack, BotSort, StrongSort\n",
    "from torchvision.models.detection import (\n",
    "    fasterrcnn_resnet50_fpn_v2,\n",
    "    FasterRCNN_ResNet50_FPN_V2_Weights as Weights\n",
    ")\n",
    "from mc_tracker import sct,mct\n",
    "from ultralytics import YOLO\n",
    "from torchvision import transforms\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "121dc4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Instruct_ReID(nn.Module):\n",
    "    def __init__(self, model, mode=\"sc\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model=model.cuda()\n",
    "        self.mode=mode\n",
    "        self.data_transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((256, 128)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                              ])\n",
    "    def forward(self,input):\n",
    "        if self.mode==\"sc\":\n",
    "            text=\"Do not change clothes\"\n",
    "\n",
    "        insreid_emb = torch.cat([self.model(self.data_transform(image).unsqueeze(0).cuda(),text, this_task_info=None)[3] for image in input],dim=0).cpu()\n",
    "        return insreid_emb\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d577fae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OSNet(nn.Module):\n",
    "    def __init__(self, model, mode=\"sc\"):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.model=model.cuda()\n",
    "        self.data_transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((256, 128)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                              ])\n",
    "    def forward(self,input):\n",
    "       \n",
    "\n",
    "        # osnet_emb = torch.cat([self.model(self.data_transform(image).unsqueeze(0).cuda()) for image in input],dim=0).cpu()\n",
    "        batch = torch.cat([self.data_transform(image).unsqueeze(0).cuda() for image in input],dim=0)\n",
    "        print(batch.shape)\n",
    "        osnet_emb = self.model(batch.cuda()).cpu()\n",
    "        return osnet_emb\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b83a46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<qdrant_client.qdrant_client.QdrantClient at 0x20de5187610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4e39f7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reid_model=OSNet(model=osnet).cuda()\n",
    "tracker = sct.SingleCameraTracker(cam_id=0,reid_model=reid_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "985f0e19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_screen_edges(detections, frame_shape, edge_margin=20):\n",
    "    H,W = frame_shape[:2]\n",
    "    edge_flages=[]\n",
    "    for (x1,y1,x2,y2) in detections[:,:4]:\n",
    "        near_edge=(x1 <= edge_margin or y1 <= edge_margin            \n",
    "                   or W-x2 <= edge_margin or H-y2 <= edge_margin) \n",
    "        edge_flages.append(near_edge)\n",
    "    return edge_flages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717c3f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 640x640 22 persons, 1 tie, 3 clocks, 65.5ms\n",
      "Speed: 2.8ms preprocess, 65.5ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 tie, 3 clocks, 53.6ms\n",
      "Speed: 5.6ms preprocess, 53.6ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 1 tie, 3 clocks, 53.0ms\n",
      "Speed: 4.2ms preprocess, 53.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 handbag, 1 tie, 4 clocks, 56.0ms\n",
      "Speed: 4.5ms preprocess, 56.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 handbag, 1 tie, 3 clocks, 48.6ms\n",
      "Speed: 3.4ms preprocess, 48.6ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 handbag, 1 tie, 3 clocks, 60.0ms\n",
      "Speed: 3.9ms preprocess, 60.0ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 handbag, 1 tie, 3 clocks, 50.9ms\n",
      "Speed: 3.9ms preprocess, 50.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 chair, 3 clocks, 49.3ms\n",
      "Speed: 3.8ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 1 umbrella, 3 clocks, 51.3ms\n",
      "Speed: 3.4ms preprocess, 51.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 2 benchs, 1 umbrella, 5 handbags, 1 tie, 3 clocks, 63.0ms\n",
      "Speed: 3.7ms preprocess, 63.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 2 benchs, 1 handbag, 1 tie, 1 chair, 3 clocks, 49.5ms\n",
      "Speed: 3.6ms preprocess, 49.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 2 benchs, 1 handbag, 1 tie, 3 clocks, 60.3ms\n",
      "Speed: 3.7ms preprocess, 60.3ms inference, 2.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 3 benchs, 2 handbags, 3 clocks, 48.2ms\n",
      "Speed: 3.6ms preprocess, 48.2ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 2 benchs, 4 handbags, 3 clocks, 48.8ms\n",
      "Speed: 3.2ms preprocess, 48.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 3 benchs, 2 handbags, 3 clocks, 50.3ms\n",
      "Speed: 3.9ms preprocess, 50.3ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 26 persons, 4 benchs, 3 clocks, 49.9ms\n",
      "Speed: 3.8ms preprocess, 49.9ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "32\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 5 benchs, 1 handbag, 3 clocks, 53.8ms\n",
      "Speed: 3.7ms preprocess, 53.8ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 23 persons, 4 benchs, 1 backpack, 4 handbags, 3 clocks, 49.9ms\n",
      "Speed: 4.4ms preprocess, 49.9ms inference, 1.0ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 23 persons, 2 benchs, 1 handbag, 3 clocks, 50.6ms\n",
      "Speed: 4.4ms preprocess, 50.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 24 persons, 2 benchs, 4 clocks, 49.2ms\n",
      "Speed: 3.8ms preprocess, 49.2ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 tie, 4 clocks, 49.3ms\n",
      "Speed: 3.4ms preprocess, 49.3ms inference, 1.2ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 2 handbags, 1 tie, 4 clocks, 49.4ms\n",
      "Speed: 3.7ms preprocess, 49.4ms inference, 0.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "267\n",
      "292\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 handbag, 2 ties, 4 clocks, 51.1ms\n",
      "Speed: 3.8ms preprocess, 51.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 2 handbags, 1 tie, 4 clocks, 50.1ms\n",
      "Speed: 3.8ms preprocess, 50.1ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "292\n",
      "\n",
      "0: 640x640 21 persons, 4 benchs, 1 handbag, 1 tie, 4 clocks, 50.0ms\n",
      "Speed: 3.8ms preprocess, 50.0ms inference, 1.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 3 handbags, 1 tie, 4 clocks, 54.3ms\n",
      "Speed: 4.5ms preprocess, 54.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 3 handbags, 1 tie, 4 clocks, 56.1ms\n",
      "Speed: 4.5ms preprocess, 56.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 3 handbags, 4 clocks, 58.6ms\n",
      "Speed: 4.6ms preprocess, 58.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 22 persons, 3 handbags, 1 tie, 4 clocks, 56.4ms\n",
      "Speed: 4.7ms preprocess, 56.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "298\n",
      "\n",
      "0: 640x640 22 persons, 2 benchs, 2 handbags, 1 tie, 4 clocks, 55.4ms\n",
      "Speed: 5.6ms preprocess, 55.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "298\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 3 handbags, 1 tie, 4 clocks, 58.0ms\n",
      "Speed: 6.1ms preprocess, 58.0ms inference, 1.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "298\n",
      "\n",
      "0: 640x640 24 persons, 2 benchs, 2 backpacks, 4 handbags, 4 clocks, 58.1ms\n",
      "Speed: 4.9ms preprocess, 58.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "298\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 1 backpack, 3 handbags, 4 clocks, 61.1ms\n",
      "Speed: 4.7ms preprocess, 61.1ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "298\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 4 handbags, 4 clocks, 55.6ms\n",
      "Speed: 4.9ms preprocess, 55.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "\n",
      "0: 640x640 27 persons, 1 bench, 1 handbag, 4 clocks, 56.9ms\n",
      "Speed: 4.5ms preprocess, 56.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "26\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "298\n",
      "\n",
      "0: 640x640 26 persons, 2 benchs, 4 handbags, 4 clocks, 57.6ms\n",
      "Speed: 4.8ms preprocess, 57.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "\n",
      "0: 640x640 27 persons, 1 bench, 6 handbags, 1 suitcase, 3 clocks, 55.6ms\n",
      "Speed: 4.9ms preprocess, 55.6ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "\n",
      "0: 640x640 26 persons, 1 bench, 1 backpack, 4 handbags, 4 clocks, 54.8ms\n",
      "Speed: 4.0ms preprocess, 54.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "247\n",
      "303\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 4 handbags, 3 clocks, 56.0ms\n",
      "Speed: 4.9ms preprocess, 56.0ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 5 handbags, 3 clocks, 53.5ms\n",
      "Speed: 4.5ms preprocess, 53.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 backpack, 3 handbags, 3 clocks, 56.4ms\n",
      "Speed: 4.1ms preprocess, 56.4ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "\n",
      "0: 640x640 20 persons, 1 bench, 2 handbags, 3 clocks, 67.0ms\n",
      "Speed: 4.7ms preprocess, 67.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 23 persons, 3 handbags, 3 clocks, 54.9ms\n",
      "Speed: 4.2ms preprocess, 54.9ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 backpack, 5 handbags, 3 clocks, 57.1ms\n",
      "Speed: 4.7ms preprocess, 57.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 1 backpack, 5 handbags, 3 clocks, 57.1ms\n",
      "Speed: 5.0ms preprocess, 57.1ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "39\n",
      "62\n",
      "145\n",
      "309\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 2 handbags, 3 clocks, 55.3ms\n",
      "Speed: 5.0ms preprocess, 55.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "309\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 4 handbags, 3 clocks, 64.7ms\n",
      "Speed: 5.6ms preprocess, 64.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "309\n",
      "\n",
      "0: 640x640 26 persons, 1 bench, 3 handbags, 3 clocks, 55.3ms\n",
      "Speed: 4.6ms preprocess, 55.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "309\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 4 clocks, 57.2ms\n",
      "Speed: 6.0ms preprocess, 57.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 24 persons, 2 benchs, 3 clocks, 53.7ms\n",
      "Speed: 4.6ms preprocess, 53.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 23 persons, 2 cars, 2 benchs, 3 clocks, 56.5ms\n",
      "Speed: 5.0ms preprocess, 56.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 26 persons, 2 benchs, 3 clocks, 55.1ms\n",
      "Speed: 4.8ms preprocess, 55.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 23 persons, 2 benchs, 1 handbag, 3 clocks, 57.7ms\n",
      "Speed: 5.2ms preprocess, 57.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 22 persons, 2 benchs, 1 handbag, 1 surfboard, 3 clocks, 56.5ms\n",
      "Speed: 4.4ms preprocess, 56.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 25 persons, 1 surfboard, 3 clocks, 72.0ms\n",
      "Speed: 4.7ms preprocess, 72.0ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "\n",
      "0: 640x640 26 persons, 1 bench, 1 surfboard, 4 clocks, 54.6ms\n",
      "Speed: 4.3ms preprocess, 54.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 4 clocks, 56.8ms\n",
      "Speed: 5.1ms preprocess, 56.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 1 bench, 4 clocks, 61.9ms\n",
      "Speed: 4.9ms preprocess, 61.9ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 2 handbags, 4 clocks, 56.5ms\n",
      "Speed: 5.1ms preprocess, 56.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 27 persons, 1 bench, 2 handbags, 4 clocks, 60.5ms\n",
      "Speed: 4.4ms preprocess, 60.5ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 1 handbag, 4 clocks, 57.1ms\n",
      "Speed: 4.4ms preprocess, 57.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 2 benchs, 3 handbags, 4 clocks, 67.9ms\n",
      "Speed: 4.8ms preprocess, 67.9ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 2 benchs, 1 handbag, 4 clocks, 58.0ms\n",
      "Speed: 5.0ms preprocess, 58.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 2 benchs, 3 clocks, 69.6ms\n",
      "Speed: 14.6ms preprocess, 69.6ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "1\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 27 persons, 1 bench, 1 handbag, 3 clocks, 56.3ms\n",
      "Speed: 4.7ms preprocess, 56.3ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 1 handbag, 3 clocks, 55.5ms\n",
      "Speed: 4.4ms preprocess, 55.5ms inference, 3.9ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "1\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "16\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 1 handbag, 3 clocks, 53.3ms\n",
      "Speed: 4.9ms preprocess, 53.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 2 benchs, 3 clocks, 56.8ms\n",
      "Speed: 4.6ms preprocess, 56.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "17\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 2 handbags, 3 clocks, 56.4ms\n",
      "Speed: 4.8ms preprocess, 56.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 27 persons, 1 bench, 1 handbag, 3 clocks, 55.2ms\n",
      "Speed: 4.4ms preprocess, 55.2ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 1 bench, 3 clocks, 56.7ms\n",
      "Speed: 4.4ms preprocess, 56.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 2 handbags, 3 clocks, 54.9ms\n",
      "Speed: 4.9ms preprocess, 54.9ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "145\n",
      "267\n",
      "\n",
      "0: 640x640 19 persons, 4 clocks, 63.7ms\n",
      "Speed: 4.6ms preprocess, 63.7ms inference, 2.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([19, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 1 handbag, 3 clocks, 54.7ms\n",
      "Speed: 5.3ms preprocess, 54.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 1 backpack, 2 handbags, 3 clocks, 63.4ms\n",
      "Speed: 4.8ms preprocess, 63.4ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 1 handbag, 3 clocks, 76.1ms\n",
      "Speed: 5.6ms preprocess, 76.1ms inference, 2.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 3 clocks, 61.0ms\n",
      "Speed: 4.9ms preprocess, 61.0ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 clocks, 56.6ms\n",
      "Speed: 5.1ms preprocess, 56.6ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 21 persons, 3 clocks, 73.1ms\n",
      "Speed: 4.6ms preprocess, 73.1ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 20 persons, 1 handbag, 1 tie, 3 clocks, 56.1ms\n",
      "Speed: 4.2ms preprocess, 56.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 3 clocks, 56.8ms\n",
      "Speed: 4.8ms preprocess, 56.8ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 3 clocks, 57.3ms\n",
      "Speed: 4.8ms preprocess, 57.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 1 handbag, 3 clocks, 60.8ms\n",
      "Speed: 4.7ms preprocess, 60.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "26\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 clocks, 54.3ms\n",
      "Speed: 4.8ms preprocess, 54.3ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 tie, 3 clocks, 55.1ms\n",
      "Speed: 4.6ms preprocess, 55.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 clocks, 56.6ms\n",
      "Speed: 4.9ms preprocess, 56.6ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "15\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 clocks, 58.7ms\n",
      "Speed: 4.7ms preprocess, 58.7ms inference, 2.1ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 clocks, 56.0ms\n",
      "Speed: 4.8ms preprocess, 56.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "16\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 22 persons, 2 benchs, 3 clocks, 55.4ms\n",
      "Speed: 4.2ms preprocess, 55.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 1 handbag, 3 clocks, 56.4ms\n",
      "Speed: 5.0ms preprocess, 56.4ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 3 clocks, 54.2ms\n",
      "Speed: 5.2ms preprocess, 54.2ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 25 persons, 1 bench, 3 clocks, 55.5ms\n",
      "Speed: 4.8ms preprocess, 55.5ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 26 persons, 1 bench, 1 handbag, 3 clocks, 55.9ms\n",
      "Speed: 9.4ms preprocess, 55.9ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([26, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 27 persons, 2 handbags, 3 clocks, 56.5ms\n",
      "Speed: 5.4ms preprocess, 56.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([27, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "11\n",
      "13\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 23 persons, 2 handbags, 3 clocks, 55.7ms\n",
      "Speed: 4.5ms preprocess, 55.7ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 3 handbags, 1 tv, 3 clocks, 53.9ms\n",
      "Speed: 4.7ms preprocess, 53.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "11\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "267\n",
      "\n",
      "0: 640x640 24 persons, 3 handbags, 1 tie, 1 tv, 4 clocks, 56.2ms\n",
      "Speed: 4.4ms preprocess, 56.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "11\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "\n",
      "0: 640x640 25 persons, 6 handbags, 1 tie, 1 tv, 4 clocks, 54.8ms\n",
      "Speed: 4.5ms preprocess, 54.8ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "\n",
      "0: 640x640 23 persons, 5 handbags, 1 tie, 4 clocks, 57.1ms\n",
      "Speed: 4.5ms preprocess, 57.1ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "8\n",
      "10\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "\n",
      "0: 640x640 23 persons, 5 handbags, 3 clocks, 62.5ms\n",
      "Speed: 5.1ms preprocess, 62.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 backpack, 4 handbags, 4 clocks, 56.2ms\n",
      "Speed: 4.7ms preprocess, 56.2ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 20 persons, 1 bench, 1 backpack, 4 handbags, 4 clocks, 55.4ms\n",
      "Speed: 5.0ms preprocess, 55.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "20\n",
      "21\n",
      "24\n",
      "31\n",
      "32\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 1 backpack, 3 handbags, 4 clocks, 58.0ms\n",
      "Speed: 4.3ms preprocess, 58.0ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 3 handbags, 4 clocks, 55.3ms\n",
      "Speed: 4.9ms preprocess, 55.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 1 backpack, 3 handbags, 4 clocks, 54.3ms\n",
      "Speed: 4.5ms preprocess, 54.3ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "4\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 1 bench, 2 backpacks, 4 handbags, 1 tie, 3 clocks, 54.7ms\n",
      "Speed: 4.7ms preprocess, 54.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 1 bench, 1 backpack, 3 handbags, 1 tie, 3 clocks, 56.7ms\n",
      "Speed: 4.7ms preprocess, 56.7ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 23 persons, 1 bench, 1 handbag, 1 tie, 4 clocks, 57.4ms\n",
      "Speed: 4.4ms preprocess, 57.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 2 handbags, 1 tie, 5 clocks, 57.8ms\n",
      "Speed: 6.2ms preprocess, 57.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 1 handbag, 4 clocks, 54.4ms\n",
      "Speed: 4.9ms preprocess, 54.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "32\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 23 persons, 1 tie, 4 clocks, 55.5ms\n",
      "Speed: 4.4ms preprocess, 55.5ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([23, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 24 persons, 1 handbag, 1 tie, 4 clocks, 54.7ms\n",
      "Speed: 5.6ms preprocess, 54.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 20 persons, 1 backpack, 2 handbags, 1 tie, 4 clocks, 57.0ms\n",
      "Speed: 4.7ms preprocess, 57.0ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 19 persons, 1 bench, 1 handbag, 1 tie, 1 suitcase, 4 clocks, 55.4ms\n",
      "Speed: 4.8ms preprocess, 55.4ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([19, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 20 persons, 1 suitcase, 4 clocks, 57.3ms\n",
      "Speed: 4.5ms preprocess, 57.3ms inference, 1.8ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "247\n",
      "267\n",
      "342\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 1 handbag, 4 clocks, 54.8ms\n",
      "Speed: 4.4ms preprocess, 54.8ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "267\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 1 handbag, 4 clocks, 56.1ms\n",
      "Speed: 4.5ms preprocess, 56.1ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "31\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "267\n",
      "346\n",
      "\n",
      "0: 640x640 20 persons, 2 handbags, 4 clocks, 58.6ms\n",
      "Speed: 4.8ms preprocess, 58.6ms inference, 2.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "267\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 2 handbags, 1 tie, 4 clocks, 54.5ms\n",
      "Speed: 4.7ms preprocess, 54.5ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 20 persons, 3 handbags, 1 tie, 4 clocks, 62.6ms\n",
      "Speed: 5.3ms preprocess, 62.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([20, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 24 persons, 3 backpacks, 2 handbags, 1 tie, 1 suitcase, 4 clocks, 56.3ms\n",
      "Speed: 4.9ms preprocess, 56.3ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 21 persons, 1 backpack, 2 handbags, 1 tie, 1 suitcase, 3 clocks, 54.6ms\n",
      "Speed: 4.4ms preprocess, 54.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([21, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 19 persons, 1 handbag, 1 tie, 3 clocks, 62.7ms\n",
      "Speed: 4.4ms preprocess, 62.7ms inference, 1.4ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([19, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 1 handbag, 1 tie, 4 clocks, 56.5ms\n",
      "Speed: 5.4ms preprocess, 56.5ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 24 persons, 2 backpacks, 1 handbag, 1 tie, 4 clocks, 54.6ms\n",
      "Speed: 4.3ms preprocess, 54.6ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 25 persons, 1 backpack, 2 handbags, 1 tv, 5 clocks, 59.1ms\n",
      "Speed: 4.7ms preprocess, 59.1ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "15\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 24 persons, 1 backpack, 3 handbags, 5 clocks, 60.8ms\n",
      "Speed: 5.3ms preprocess, 60.8ms inference, 1.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "18\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 25 persons, 1 backpack, 3 handbags, 4 clocks, 65.2ms\n",
      "Speed: 4.7ms preprocess, 65.2ms inference, 1.5ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([25, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "18\n",
      "19\n",
      "20\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 22 persons, 4 handbags, 1 suitcase, 5 clocks, 61.9ms\n",
      "Speed: 5.6ms preprocess, 61.9ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([22, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n",
      "\n",
      "0: 640x640 24 persons, 1 backpack, 1 handbag, 5 clocks, 57.8ms\n",
      "Speed: 4.7ms preprocess, 57.8ms inference, 1.6ms postprocess per image at shape (1, 3, 640, 640)\n",
      "torch.Size([24, 3, 256, 128])\n",
      "2\n",
      "4\n",
      "5\n",
      "7\n",
      "9\n",
      "10\n",
      "11\n",
      "14\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "24\n",
      "28\n",
      "39\n",
      "62\n",
      "123\n",
      "145\n",
      "247\n",
      "346\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Load detector with pretrained weights and preprocessing transforms\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "data_transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((256, 128)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                              ])\n",
    "\n",
    "det_across_frames=[]\n",
    "def crop_frame(frame, box):\n",
    "    x1,y1,x2,y2=box.astype(np.int32)\n",
    "\n",
    "    cropped = frame[y1:y2,x1:x2]\n",
    "    return cropped\n",
    "\n",
    "def process_crops(crops, transform):\n",
    "    transformed_crops = torch.stack([transform(crop) for crop in crops])\n",
    "    final_crops = transformed_crops.unsqueeze(2).repeat(1,1,8,1,1)\n",
    "    return final_crops\n",
    "# writer = cv2.VideoWriter(\"osnet_x0_25_msmt17.mp4\",cv2.VideoWriter_fourcc(*\"mpv4\"),25,(1440,710))\n",
    "weights = Weights.DEFAULT\n",
    "# detector = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
    "# detector.to(device).eval()\n",
    "# transform = weights.transforms()\n",
    "\n",
    "detector = YOLO(r\"rtdetr-l.pt\")\n",
    "detector.to(device).eval()\n",
    "transform = weights.transforms()\n",
    "# vslaclip_reid = torch.load(\"vslaclip.pth\").to(device).eval()\n",
    "\n",
    "# Initialize tracker\n",
    "# tracker = BotSort(reid_weights=Path('clip_market1501.pt'), device=device, half=False)\n",
    "\n",
    "\n",
    "# Start video capture\n",
    "video_path = r'C:\\Users\\hthek\\OneDrive\\Desktop\\Collected Dataset\\Compressed_Vids\\Vid_1.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "count=0\n",
    "with torch.inference_mode():\n",
    "    while True:\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB and prepare for detector\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "   \n",
    "\n",
    "        # Run detection\n",
    "        if count%1==0:\n",
    "            output = detector(frame)[0].boxes\n",
    "            labels = output.cls.cpu().numpy()\n",
    "            only_people = labels == 0\n",
    "            scores = output.conf.cpu().numpy()\n",
    "            scores = scores[only_people]\n",
    "            keep = scores >= 0.25\n",
    "            filtered_scores = scores[keep]\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # Prepare detections for tracking\n",
    "            boxes = output.xyxy.cpu().numpy()[only_people][keep]\n",
    "            # labels = labels[only_people][keep]\n",
    "            # detections = np.concatenate([boxes, filtered_scores[:, None], labels[:, None]], axis=1)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        # Update tracker and draw results\n",
    "        #   INPUT:  M X (x, y, x, y, conf, cls)\n",
    "        #   OUTPUT: M X (x, y, x, y, id, conf, cls, ind)\n",
    "        H, W, _ = frame.shape\n",
    "        boundary_coord = [0, 0, W, H]\n",
    "        screen_edges = get_screen_edges(boxes,frame.shape)\n",
    "        boxes=boxes.astype(np.int32)\n",
    "        tracker.process(frame,boxes,screen_edges,boundary_coord)\n",
    "        for obj in tracker.get_tracked_objects():\n",
    "            if obj.display:\n",
    "                x1,y1,x2,y2 = obj.rect\n",
    "                id = obj.label.split(\"-\")[-1]\n",
    "                int_id = int(id)\n",
    "                color = (max(10*int_id,255),max(100+int_id,255),min(255-5*int_id,0))\n",
    "                print(id)\n",
    "                cv2.rectangle(frame,(x1,y1),(x2,y2),color=color,thickness=2)\n",
    "                cv2.putText(frame,\"id: \"+id,(x1,y1-5),fontFace=cv2.FONT_HERSHEY_COMPLEX,fontScale=0.5,color=color)\n",
    "        \n",
    "        \n",
    "\n",
    "        # writer.write(frame)\n",
    "        # if count>5:\n",
    "        #     break\n",
    "        # count+=1\n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        cv2.imshow('BoXMOT + Torchvision', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4b5bfa",
   "metadata": {},
   "source": [
    "## MCT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35acc0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load detector with pretrained weights and preprocessing transforms\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "sct_config = {\n",
    "    'match_threshold': 0.25,\n",
    "    'n_clusters': 4,\n",
    "    'clust_init_dis_thresh': 0.1,\n",
    "    'time_window': 10\n",
    "}\n",
    "\n",
    "\n",
    "data_transform=transforms.Compose([\n",
    "                              transforms.ToPILImage(),\n",
    "                              transforms.Resize((256, 128)),\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]),\n",
    "                              ])\n",
    "\n",
    "det_across_frames=[]\n",
    "def crop_frame(frame, box):\n",
    "    x1,y1,x2,y2=box.astype(np.int32)\n",
    "\n",
    "    cropped = frame[y1:y2,x1:x2]\n",
    "    return cropped\n",
    "\n",
    "def process_crops(crops, transform):\n",
    "    transformed_crops = torch.stack([transform(crop) for crop in crops])\n",
    "    final_crops = transformed_crops.unsqueeze(2).repeat(1,1,8,1,1)\n",
    "    return final_crops\n",
    "# writer = cv2.VideoWriter(\"osnet_x0_25_msmt17.mp4\",cv2.VideoWriter_fourcc(*\"mpv4\"),25,(1440,710))\n",
    "weights = Weights.DEFAULT\n",
    "# detector = fasterrcnn_resnet50_fpn_v2(weights=weights, box_score_thresh=0.5)\n",
    "# detector.to(device).eval()\n",
    "# transform = weights.transforms()\n",
    "\n",
    "detector = YOLO(r\"rtdetr-l.pt\")\n",
    "detector.to(device).eval()\n",
    "transform = weights.transforms()\n",
    "# vslaclip_reid = torch.load(\"vslaclip.pth\").to(device).eval()\n",
    "\n",
    "# Initialize tracker\n",
    "# tracker = BotSort(reid_weights=Path('clip_market1501.pt'), device=device, half=False)\n",
    "\n",
    "\n",
    "# Start video capture\n",
    "reid_model=Instruct_ReID(model=ins_ReID).cuda()\n",
    "mct1=mct.MultiCameraTracker(reid_model=reid_model,cam_id=0,sct_config=sct_config)\n",
    "mct2=mct.MultiCameraTracker(reid_model=reid_model,cam_id=1,sct_config=sct_config)\n",
    "video1_path = r'C:\\Users\\hthek\\OneDrive\\Desktop\\Collected Dataset\\Compressed_Vids\\Vid_1.mp4'\n",
    "video2_path = r'C:\\Users\\hthek\\OneDrive\\Desktop\\Collected Dataset\\Compressed_Vids\\Vid_2.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "count=0\n",
    "with torch.inference_mode():\n",
    "    while True:\n",
    "        \n",
    "        success, frame = cap.read()\n",
    "        if not success:\n",
    "            break\n",
    "\n",
    "        # Convert frame to RGB and prepare for detector\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "   \n",
    "\n",
    "        # Run detection\n",
    "        if count%1==0:\n",
    "            output = detector(frame)[0].boxes\n",
    "            labels = output.cls.cpu().numpy()\n",
    "            only_people = labels == 0\n",
    "            scores = output.conf.cpu().numpy()\n",
    "            scores = scores[only_people]\n",
    "            keep = scores >= 0.25\n",
    "            filtered_scores = scores[keep]\n",
    "            \n",
    "\n",
    "        \n",
    "        \n",
    "        # Prepare detections for tracking\n",
    "            boxes = output.xyxy.cpu().numpy()[only_people][keep]\n",
    "            # labels = labels[only_people][keep]\n",
    "            # detections = np.concatenate([boxes, filtered_scores[:, None], labels[:, None]], axis=1)\n",
    "        \n",
    "       \n",
    "            \n",
    "        \n",
    "          \n",
    "            \n",
    "        \n",
    "       \n",
    "            \n",
    "            \n",
    "            \n",
    "        \n",
    "\n",
    "        # Update tracker and draw results\n",
    "        #   INPUT:  M X (x, y, x, y, conf, cls)\n",
    "        #   OUTPUT: M X (x, y, x, y, id, conf, cls, ind)\n",
    "        H, W, _ = frame.shape\n",
    "        boundary_coord = [0, 0, W, H]\n",
    "        screen_edges = get_screen_edges(boxes,frame.shape)\n",
    "        boxes=boxes.astype(np.int32)\n",
    "        tracker.process(frame,boxes,screen_edges,boundary_coord)\n",
    "        for obj in tracker.get_tracked_objects():\n",
    "            x1,y1,x2,y2 = obj.rect\n",
    "            cv2.rectangle(frame,(x1,y1),(x2,y2),color=(0,0,255),thickness=2)\n",
    "        \n",
    "        \n",
    "\n",
    "        # writer.write(frame)\n",
    "        # if count>5:\n",
    "        #     break\n",
    "        # count+=1\n",
    "        \n",
    "       \n",
    "\n",
    "        \n",
    "        \n",
    "        cv2.imshow('BoXMOT + Torchvision', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "        \n",
    "\n",
    "# Clean up\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "80ab2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TrackedObj(rect=array([829, 572, 912, 709], dtype=int32), label='0-1', display=True)\n",
      "TrackedObj(rect=array([669, 479, 695, 553], dtype=int32), label='0-2', display=True)\n",
      "TrackedObj(rect=array([310, 363, 361, 467], dtype=int32), label='0-3', display=True)\n",
      "TrackedObj(rect=array([127, 372, 165, 462], dtype=int32), label='0-4', display=True)\n",
      "TrackedObj(rect=array([366, 376, 394, 459], dtype=int32), label='0-5', display=True)\n",
      "TrackedObj(rect=array([172, 355, 217, 451], dtype=int32), label='0-6', display=True)\n",
      "TrackedObj(rect=array([828, 423, 870, 541], dtype=int32), label='0-7', display=True)\n",
      "TrackedObj(rect=array([205, 380, 271, 474], dtype=int32), label='0-8', display=True)\n",
      "TrackedObj(rect=array([735, 435, 770, 518], dtype=int32), label='0-9', display=True)\n",
      "TrackedObj(rect=array([401, 380, 439, 475], dtype=int32), label='0-10', display=True)\n",
      "TrackedObj(rect=array([759, 344, 794, 415], dtype=int32), label='0-11', display=True)\n",
      "TrackedObj(rect=array([679, 323, 710, 407], dtype=int32), label='0-12', display=True)\n",
      "TrackedObj(rect=array([828, 528, 887, 695], dtype=int32), label='0-14', display=True)\n",
      "TrackedObj(rect=array([281, 495, 321, 519], dtype=int32), label='0-15', display=True)\n",
      "TrackedObj(rect=array([780, 322, 806, 385], dtype=int32), label='0-16', display=True)\n",
      "TrackedObj(rect=array([896, 479, 954, 624], dtype=int32), label='0-17', display=True)\n",
      "TrackedObj(rect=array([733, 434, 768, 557], dtype=int32), label='0-18', display=True)\n",
      "TrackedObj(rect=array([839, 530, 887, 620], dtype=int32), label='0-19', display=True)\n",
      "TrackedObj(rect=array([925, 521, 961, 674], dtype=int32), label='0-20', display=True)\n",
      "TrackedObj(rect=array([912, 479, 957, 619], dtype=int32), label='0-22', display=True)\n",
      "TrackedObj(rect=array([866, 333, 889, 416], dtype=int32), label='0-24', display=False)\n",
      "TrackedObj(rect=array([932, 406, 966, 501], dtype=int32), label='0-25', display=False)\n",
      "TrackedObj(rect=array([699, 463, 756, 585], dtype=int32), label='0-26', display=False)\n",
      "TrackedObj(rect=array([898, 479, 957, 631], dtype=int32), label='0-29', display=False)\n",
      "TrackedObj(rect=array([673, 511, 716, 593], dtype=int32), label='0-30', display=False)\n",
      "TrackedObj(rect=array([736, 435, 757, 489], dtype=int32), label='0-31', display=False)\n"
     ]
    }
   ],
   "source": [
    "for i in tracker.get_tracked_objects():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f171f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tracking",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
